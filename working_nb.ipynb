{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9414b0c3",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from env import user, password, host\n",
    "from scipy import stats\n",
    "from scipy.stats import levene, ttest_ind\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ?from pandas_profiling import ProfileReport\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Visualizing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# default pandas decimal number display format\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb8b3a",
   "metadata": {},
   "source": [
    "# Purpose of the this project\n",
    "### I am trying to classify the city services request from the calls received.  The metric to be used is accuracy.  This will enable better customer service by providing useful data analysis to help determine funding and ataffing allocations to departments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623a77a",
   "metadata": {},
   "source": [
    "# ACQUIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_311_data():\n",
    "    ''' \n",
    "    This function reads in a csv held in the same repository folder\n",
    "    '''\n",
    "    df = pd.read_csv('data_311.csv')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ca99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=get_311_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ee8ef",
   "metadata": {},
   "source": [
    "# SUMMARIZE DATA \n",
    "#### WHAT DO WE HAVE HERE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return first df rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9871d9",
   "metadata": {},
   "source": [
    "#### Looks like a log of calls for city service departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb440f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return last df rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afad06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b2878",
   "metadata": {},
   "source": [
    "#### Need to see if columns have REDUNDANCY and see if column will be useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give numerical data, stats, min, max, quartile info\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca360151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list number columns and rows, data types,and non null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033b28a",
   "metadata": {},
   "source": [
    "#### Most are object but case_id is an integer and num_days_late is a float. Also, I might want to organize it by the case_id, or by date, maybe by case_opened_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d94a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number null values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing[missing >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "18333/855269"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2ca69",
   "metadata": {},
   "source": [
    "#### But it isn't a large amount (only 2.1%), so I will consider dropping columns with nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a6520",
   "metadata": {},
   "source": [
    "#### NOTE for the cells below. I know that the following is a bit much, but I really need to see how the data is formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .unique and .nunique with object columns.\n",
    "\n",
    "obj_cols = df.columns[[df[col].dtype == 'O' for col in df.columns]]\n",
    "for col in obj_cols:\n",
    "    print(df[col])\n",
    "    print(df[col].unique()) #prints actual values in column\n",
    "    print(df[col].nunique()) #prints number of unique values\n",
    "    print('----------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see period of time \n",
    "df.case_opened_date.min(), df.case_opened_date.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .value_counts() with object columns to get actual and percent values in categorical columns\n",
    "\n",
    "obj_cols = df.columns[[df[col].dtype == 'O' for col in df.columns]]\n",
    "for col in obj_cols:\n",
    "    print(df[col].value_counts()) #prints actual count\n",
    "    print(df[col].value_counts(normalize=True, dropna=False)) #prints %\n",
    "    print('----------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1ce14",
   "metadata": {},
   "source": [
    "# PREP & CLEAN\n",
    "#### drop duplicates, get rid of nulls, drop some columns, encode columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ad474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows containing duplicate data\n",
    "duplicate_rows_df = df[df.duplicated()]\n",
    "print('number of duplicate rows: ', duplicate_rows_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c79fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the missing values.\n",
    "df = df.dropna() \n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['council_district'].str.contains('YES|Code Enforcement Services')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d85b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['council_district'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdd1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['council_district'] = df['council_district'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['council_district'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead6fd32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m late_mean\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_days_late\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "late_mean=df['num_days_late'].mean()\n",
    "late_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08634dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['source_id', 'case_id', 'SLA_due_date', 'case_closed', 'SLA_days', 'case_status', 'dept_name',\n",
    "                    'dept_subject_to_SLA', 'index','source_username'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31706c",
   "metadata": {},
   "source": [
    "### Number of days late has both negative and positive values. Requests have to be completed in a timely manner, so if it is a negative number then it was completed before the due date, so I am going to encode 'case_late' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064451cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['case_late_encoded'] = df.case_late.map({'YES': 1, 'NO':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c6984",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46069ebe",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis(EDA)\n",
    "What kind of analysis are we doing?\n",
    "Univariate\n",
    "Bivariate\n",
    "\n",
    "\n",
    "1. Check how many numerical features may be there \n",
    "    *We try to see numerical features with the help of drawing of different diagrams like histogram , pdf function )\n",
    "\n",
    "2. Check categorical features\n",
    "    *In categorical features we are try the analyze category features like how many category feature may be there\n",
    "    *In those feature may be we have multiple categories\n",
    "\n",
    "3. Checking for missing values or for duplicated rows\n",
    "    *For checking checking missing value just try to visualize all the graphs all these graphs )\n",
    "\n",
    "4. Checking for outliers\n",
    "    *We can simply check whether there are outliers with help of box plot)\n",
    "\n",
    "5. Handling Missing values\n",
    "    *There are many ways to handel missing value like mean , mode , median and many more\n",
    "\n",
    "6. Handling imbalanced Data Set\n",
    "\n",
    "7. Treating Outliers\n",
    "\n",
    "8. Scaling down the data (standardization and normalization)\n",
    "\n",
    "9. Converting our categorical features into numerical features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dcb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_311_data(df):\n",
    "    '''\n",
    "    This function performs split on 311 data, stratify case_late_encoded\n",
    "    Returns train, validate, and test dfs.\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=df.case_late_encoded)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.case_late_encoded)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test= split_311_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tab = pd.crosstab(index = train['case_late_encoded'],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "\n",
    "my_tab.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tab = pd.crosstab(index = train['standardized_dept_name'],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "\n",
    "my_tab.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tab = pd.crosstab(index = train['council_district'],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "\n",
    "my_tab.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tab = pd.crosstab(index = train['case_late'],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "\n",
    "my_tab.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a746c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tab = pd.crosstab(index = train['dept_division'],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "\n",
    "my_tab.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760bd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out distributions of numeric columns.\n",
    "\n",
    "num_cols = train.columns[[train[col].dtype == 'float64' for col in train.columns]]\n",
    "for col in num_cols:\n",
    "    plt.hist(train[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two-way table\n",
    "grouped = train.groupby(['case_late','dept_division'])\n",
    "grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4014218",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(index=train['case_late'], columns=train['service_request_type'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beec566",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.plot(kind='bar', figsize=(8,8), stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8941338",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(index=train['case_late'], columns=train['council_district'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae90cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.plot(kind='bar', figsize=(8,8), stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(index=train['case_late'], columns=train['dept_division'])\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.plot(kind='bar', figsize=(8,8), stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d790bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.violinplot(x='standardized_dept_name', y='case_late_encoded' , data=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49674704",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.violinplot(x='standardized_dept_name', y='num_days_late' , data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.boxplot(train['num_days_late'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.barplot(train['num_days_late'], train['standardized_dept_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.barplot(train['num_days_late'], train['council_district'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.barplot(train['num_days_late'], train['dept_division'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ea3f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(train['standardized_dept_name'], train['num_days_late'])\n",
    "\n",
    "plt.title('Days Late and Departments')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Days Late')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cf499",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(train['council_district'], train['num_days_late'])\n",
    "\n",
    "plt.title('Days Late and Council Districts')\n",
    "plt.xlabel('Council District')\n",
    "plt.ylabel('Days Late')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get numerical data, stats, min, max, quartile info for each district\n",
    "by_council = train.groupby(['council_district'])\n",
    "by_council.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dept = train.groupby(['standardized_dept_name'])\n",
    "by_dept.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_request= train.groupby(['service_request_type'])\n",
    "by_request.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcafd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= sns.FacetGrid(train, col='standardized_dept_name', row='case_late_encoded')\n",
    "a.map(plt.hist, 'num_days_late')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= sns.FacetGrid(train, col='council_district', row='case_late_encoded')\n",
    "a.map(plt.hist, 'num_days_late')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77765a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= sns.FacetGrid(train, col='dept_division', row='case_late_encoded')\n",
    "a.map(plt.hist, 'num_days_late')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= sns.FacetGrid(train, col='service_request_type', row='case_late_encoded')\n",
    "a.map(plt.hist, 'num_days_late')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b12bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= sns.FacetGrid(train, col='standardized_dept_name', row='case_late_encoded')\n",
    "a.map(plt.hist, 'num_days_late')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484668a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mscatterplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_days_late\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdept_division\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mtrain, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_late_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.scatterplot(x='num_days_late', y='dept_division', data=train, hue='case_late_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.average=df['num_days_late'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37869cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d93a6c5f",
   "metadata": {},
   "source": [
    "Statistical Normality Tests\n",
    "There are many statistical tests that we can use to quantify whether a sample of data looks as though it was drawn from a Gaussian distribution.\n",
    "\n",
    "Each test makes different assumptions and considers different aspects of the data.\n",
    "\n",
    "We will look at 3 commonly used tests in this section that you can apply to your own data samples.\n",
    "\n",
    "Interpretation of a Test\n",
    "Before you can apply the statistical tests, you must know how to interpret the results.\n",
    "\n",
    "Each test will return at least two things:\n",
    "    *Statistic: A quantity calculated by the test that can be interpreted in the context of the test via comparing it\n",
    "    to critical values from the distribution of the test statistic.\n",
    "\n",
    "    *p-value: Used to interpret the test, in this case whether the sample was drawn from a Gaussian distribution.\n",
    "\n",
    "Each test calculates a test-specific statistic. This statistic can aid in the interpretation of the result, although it may require a deeper proficiency with statistics and a deeper knowledge of the specific statistical test. Instead, the p-value can be used to quickly and accurately interpret the statistic in practical applications.\n",
    "\n",
    "The tests assume that that the sample was drawn from a Gaussian distribution. \n",
    "    *Technically this is called the null hypothesis, or H0. A threshold level is chosen called alpha, typically 5% (or \n",
    "    0.05), that is used to interpret the p-value.\n",
    "\n",
    "In the SciPy implementation of these tests, you can interpret the p value as follows.\n",
    "    p <= alpha: reject H0, not normal.\n",
    "    p > alpha: fail to reject H0, normal.\n",
    "\n",
    "This means that, in general, we are seeking results with a larger p-value to confirm that our sample was likely drawn from a Gaussian distribution.\n",
    "\n",
    "A result above 5% does not mean that the null hypothesis is true. It means that it is very likely true given available evidence. The p-value is not the probability of the data fitting a Gaussian distribution; it can be thought of as a value that helps us interpret the statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb44733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
